<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Blaze - High-Performance Full-Text Search Engine in Go</title>
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css"
      rel="stylesheet"
    />
    <style>
      :root {
        --bg-color: #ffffff;
        --text-color: #333333;
        --border-color: #eeeeee;
        --code-bg: #f8f8f8;
        --accent: #333333;
      }

      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
          "Helvetica Neue", Arial, sans-serif;
        line-height: 1.6;
        color: var(--text-color);
        background: var(--bg-color);
      }

      .container {
        max-width: 900px;
        margin: 0 auto;
        padding: 2rem;
      }

      header {
        margin-bottom: 3rem;
        padding-bottom: 2rem;
        border-bottom: 2px solid var(--border-color);
      }

      h1 {
        font-size: 3rem;
        font-weight: 700;
        margin-bottom: 1rem;
        letter-spacing: -0.02em;
      }

      .subtitle {
        font-size: 1.25rem;
        color: #666;
        margin-bottom: 1.5rem;
        line-height: 1.5;
      }

      .header-buttons {
        display: flex;
        gap: 1rem;
        flex-wrap: wrap;
      }

      .btn {
        display: inline-block;
        padding: 0.75rem 1.5rem;
        border: 2px solid var(--accent);
        text-decoration: none;
        color: var(--accent);
        font-weight: 600;
        transition: all 0.2s;
        border-radius: 4px;
      }

      .btn:hover {
        background: var(--accent);
        color: white;
      }

      .btn-primary {
        background: var(--accent);
        color: white;
      }

      .btn-primary:hover {
        background: white;
        color: var(--accent);
      }

      h2 {
        font-size: 2rem;
        margin: 3rem 0 1rem;
        padding-bottom: 0.5rem;
        border-bottom: 1px solid var(--border-color);
      }

      h3 {
        font-size: 1.5rem;
        margin: 2rem 0 1rem;
      }

      h4 {
        font-size: 1.25rem;
        margin: 1.5rem 0 0.75rem;
      }

      p {
        margin: 1rem 0;
        line-height: 1.7;
      }

      ul,
      ol {
        margin: 1rem 0 1rem 2rem;
        line-height: 1.7;
      }

      li {
        margin: 0.5rem 0;
      }

      pre {
        background: var(--code-bg);
        padding: 1.5rem;
        overflow-x: auto;
        border-radius: 6px;
        margin: 1.5rem 0;
        border: 1px solid var(--border-color);
      }

      code {
        background: #f0f0f0;
        padding: 0.2rem 0.4rem;
        border-radius: 3px;
        font-family: "Monaco", "Consolas", "Courier New", monospace;
        font-size: 0.9em;
      }

      pre code {
        background: transparent;
        padding: 0;
        font-size: 0.85rem;
        line-height: 1.5;
      }

      table {
        border-collapse: collapse;
        width: 100%;
        margin: 1.5rem 0;
      }

      th,
      td {
        border: 1px solid var(--border-color);
        padding: 0.75rem;
        text-align: left;
      }

      th {
        background: var(--code-bg);
        font-weight: 600;
      }

      .features {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
        gap: 1.5rem;
        margin: 2rem 0;
      }

      .feature-card {
        padding: 1.5rem;
        border: 1px solid var(--border-color);
        border-radius: 6px;
      }

      .feature-card h4 {
        margin-top: 0;
      }

      .ascii-art {
        background: white;
        border: 1px solid var(--border-color);
        padding: 1rem;
        font-family: monospace;
        font-size: 0.75rem;
        line-height: 1.3;
        overflow-x: auto;
        white-space: pre;
      }

      .highlight-box {
        background: #f8f9fa;
        border-left: 4px solid var(--accent);
        padding: 1rem 1.5rem;
        margin: 1.5rem 0;
      }

      footer {
        margin-top: 4rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-color);
        text-align: center;
        color: #666;
      }

      /* Prism theme overrides */
      code[class*="language-"],
      pre[class*="language-"] {
        color: #383a42;
        text-shadow: none;
      }

      .token.comment {
        color: #a0a1a7;
        font-style: italic;
      }

      .token.function,
      .token.keyword {
        color: #a626a4;
      }

      .token.string {
        color: #50a14f;
      }

      .token.number {
        color: #986801;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        h1 {
          font-size: 2rem;
        }

        h2 {
          font-size: 1.5rem;
        }

        .features {
          grid-template-columns: 1fr;
        }
      }
    </style>
  </head>

  <body>
    <div class="container">
      <header>
        <h1>Blaze</h1>
        <p class="subtitle">
          A high-performance, full-text search engine in Go featuring an
          inverted index with skip lists, phrase search, proximity ranking, and
          advanced text analysis.
        </p>
        <div class="header-buttons">
          <a href="https://github.com/wizenheimer/blaze" class="btn"
            >View on GitHub</a
          >
          <a
            href="https://pkg.go.dev/github.com/wizenheimer/blaze"
            class="btn btn-primary"
            >Documentation →</a
          >
        </div>
      </header>

      <section>
        <h2>Overview</h2>
        <p>
          Blaze is a Go engine that provides fast, full-text search capabilities
          through an inverted index implementation. It's designed for
          applications that need to search through text documents efficiently
          without relying on external search engines.
        </p>

        <div class="features">
          <div class="feature-card">
            <h4>Inverted Index</h4>
            <p>Maps terms to document positions for instant lookups</p>
          </div>
          <div class="feature-card">
            <h4>Skip Lists</h4>
            <p>Probabilistic data structure providing O(log n) operations</p>
          </div>
          <div class="feature-card">
            <h4>Advanced Search</h4>
            <p>Phrase search, proximity ranking, and boolean queries</p>
          </div>
          <div class="feature-card">
            <h4>Text Analysis</h4>
            <p>Tokenization, stemming, stopword filtering, and normalization</p>
          </div>
          <div class="feature-card">
            <h4>Thread-Safe</h4>
            <p>Concurrent indexing with mutex protection</p>
          </div>
          <div class="feature-card">
            <h4>Serialization</h4>
            <p>Efficient binary format for persistence</p>
          </div>
        </div>
      </section>

      <section>
        <h2>Installation</h2>
        <pre><code class="language-bash">go get github.com/wizenheimer/blaze</code></pre>
      </section>

      <section>
        <h2>Quick Start</h2>
        <pre><code class="language-go">package main

import (
    "fmt"
    "github.com/wizenheimer/blaze"
)

func main() {
    // Create a new inverted index
    idx := blaze.NewInvertedIndex()

    // Index some documents
    idx.Index(1, "The quick brown fox jumps over the lazy dog")
    idx.Index(2, "A quick brown dog runs fast")
    idx.Index(3, "The lazy cat sleeps all day")

    // Search for documents containing "quick" and "brown"
    matches := idx.RankProximity("quick brown", 10)

    // Print results
    for _, match := range matches {
        fmt.Printf("Document %d (score: %.2f)\n",
            int(match.Offsets[0].DocumentID),
            match.Score)
    }
}</code></pre>

        <p><strong>Output:</strong></p>
        <pre><code>Document 2 (score: 1.00)
Document 1 (score: 0.50)</code></pre>
      </section>

      <section>
        <h2>Core Concepts</h2>

        <h3>Inverted Index</h3>
        <p>
          An inverted index is like the index at the back of a book. Instead of
          scanning every document to find a word, the index tells you exactly
          where each word appears.
        </p>

        <div class="highlight-box">
          <p><strong>Example Documents:</strong></p>
          <code>
            Doc 1: "the quick brown fox"<br />
            Doc 2: "the lazy dog"<br />
            Doc 3: "quick brown dogs"
          </code>
        </div>

        <div class="ascii-art">
          ┌─────────┬────────────────────────────────────┐ │ Token │ Posting
          List │ ├─────────┼────────────────────────────────────┤ │ "quick" │ →
          [Doc1:Pos1] → [Doc3:Pos0] │ │ "brown" │ → [Doc1:Pos2] → [Doc3:Pos1] │
          │ "fox" │ → [Doc1:Pos3] │ │ "lazy" │ → [Doc2:Pos1] │ │ "dog" │ →
          [Doc2:Pos2] │ │ "dogs" │ → [Doc3:Pos2] │
          └─────────┴────────────────────────────────────┘
        </div>

        <p><strong>Visual Representation:</strong></p>
        <div class="ascii-art">
          Inverted Index ┌──────────┐ │ Map │ │ [string] │ │ SkipList │
          └────┬─────┘ │ ┌────────────────┼────────────────┐ │ │ │ ▼ ▼ ▼ "quick"
          "brown" "fox" SkipList SkipList SkipList ┌──────┐ ┌──────┐ ┌──────┐ │
          HEAD │ │ HEAD │ │ HEAD │ └──┬───┘ └──┬───┘ └──┬───┘ │ │ │ ▼ ▼ ▼
          ┌──────┐ ┌──────┐ ┌──────┐ │Doc1:1│ │Doc1:2│ │Doc1:3│ └──┬───┘
          └──┬───┘ └──────┘ │ │ ▼ ▼ ┌──────┐ ┌──────┐ │Doc3:0│ │Doc3:1│ └──────┘
          └──────┘
        </div>

        <p><strong>Benefits:</strong></p>
        <ul>
          <li>Instant term lookups (no document scanning)</li>
          <li>Phrase search via position checking</li>
          <li>Proximity ranking by measuring distances</li>
          <li>Efficient boolean queries (AND, OR, NOT)</li>
        </ul>

        <h3>Skip Lists</h3>
        <p>
          A skip list is a probabilistic data structure that maintains sorted
          data with O(log n) average time complexity for search, insertion, and
          deletion.
        </p>

        <div class="ascii-art">
          Skip List with Multiple Levels (Express Lanes)
          ═══════════════════════════════════════════════════════════════ Level
          3: HEAD ────────────────────────────────────────> [30] ──────> NULL ↓
          ↓ Level 2: HEAD ───────────────────> [15] ─────────────> [30] ──────>
          NULL ↓ ↓ ↓ Level 1: HEAD ──────> [10] ──────> [15] ──────> [20] ─>
          [30] ──────> NULL ↓ ↓ ↓ ↓ ↓ Level 0: HEAD → [5] → [10] → [15] → [20] →
          [25] → [30] → [35] → NULL (ALL NODES AT LEVEL 0)
        </div>

        <div class="highlight-box">
          <p><strong>Node Structure:</strong></p>
          <div class="ascii-art">
            ┌───────┐ │ Node │ Each node has a "tower" of forward pointers
            ├───────┤ │ Key │ Example: Node [15] ├───────┤ │ Lvl 3 │ ──> [30]
            (skip far ahead) │ Lvl 2 │ ──> [30] (skip ahead) │ Lvl 1 │ ──> [20]
            (skip a little) │ Lvl 0 │ ──> [20] (next node) └───────┘
          </div>
        </div>

        <p><strong>How Heights are Assigned (Probabilistic):</strong></p>
        <div class="ascii-art">
          Coin Flip Algorithm: ┌─────────┬─────────────┬─────────────┐ │ Height
          │ Probability │ Visual │ ├─────────┼─────────────┼─────────────┤ │ 1 │
          50% │ ▓▓▓▓▓ │ │ 2 │ 25% │ ▓▓▓ │ │ 3 │ 12.5% │ ▓▓ │ │ 4 │ 6.25% │ ▓ │
          └─────────┴─────────────┴─────────────┘ For 1000 nodes, expected
          distribution: Level 0: ~1000 nodes (all)
          ████████████████████████████████████████ Level 1: ~500 nodes
          ████████████████████ Level 2: ~250 nodes ██████████ Level 3: ~125
          nodes █████ Level 4: ~62 nodes ██
        </div>

        <p><strong>Search Algorithm Example</strong> (finding 20):</p>
        <div class="ascii-art">
          Step-by-Step Search for Key = 20: Level 3: [HEAD]
          ───────────────────────────────> [30] (30 > 20, drop down) ↓ Level 2:
          [HEAD] ──────────────> [15] ─────────> [30] (15 < 20, advance) ↓ Level
          2: [15] ─────────> [30] (30 > 20, drop down) ↓ Level 1: [15] ──> [20]
          (20 = 20, FOUND!) ^^^^ Journey Recorded:
          ┌───────────┬─────────────────┐ │ Level 3 │ HEAD │ Predecessor at each
          level │ Level 2 │ [15] │ Used for insertions/deletions │ Level 1 │
          [15] │ │ Level 0 │ [15] │ └───────────┴─────────────────┘ Steps: 1.
          Start at HEAD, Level 3 2. Level 3: Move to 30? No (30 > 20), drop to
          Level 2 3. Level 2: Move to 15? Yes (15 < 20), advance to 15 4. Level
          2: Move to 30? No (30 > 20), drop to Level 1 5. Level 1: Move to 20?
          Yes! Found it! Time Complexity: O(log n) on average
        </div>

        <p><strong>Why Skip Lists?</strong></p>
        <ul>
          <li>O(log n) operations without complex balancing</li>
          <li>Simpler than AVL or Red-Black trees</li>
          <li>Better cache locality than trees</li>
          <li>Easier to make lock-free for concurrency</li>
          <li>Used in Redis, LevelDB, and other databases</li>
        </ul>

        <h3>Text Analysis Pipeline</h3>
        <p>
          Blaze transforms raw text into searchable tokens through a multi-stage
          pipeline:
        </p>

        <div class="ascii-art">
          ┌─────────────────────────────────────────────────────────────────────┐
          │ Text Analysis Pipeline │
          └─────────────────────────────────────────────────────────────────────┘
          │ ▼ ┌────────────────────────────────────────┐ │ 1. Tokenization │ │
          Split on non-alphanumeric chars │
          └────────────────┬───────────────────────┘ ▼
          ┌────────────────────────────────────────┐ │ 2. Lowercasing │ │
          Normalize case ("Quick" → "quick") │
          └────────────────┬───────────────────────┘ ▼
          ┌────────────────────────────────────────┐ │ 3. Stopword Filtering │ │
          Remove common words (the, a, is) │
          └────────────────┬───────────────────────┘ ▼
          ┌────────────────────────────────────────┐ │ 4. Length Filtering │ │
          Remove tokens < 2 chars │ └────────────────┬───────────────────────┘ ▼
          ┌────────────────────────────────────────┐ │ 5. Stemming
          (Snowball/Porter2) │ │ Reduce to root ("running" → "run") │
          └────────────────┬───────────────────────┘ ▼ Final Tokens
        </div>

        <div class="highlight-box">
          <p><strong>Example Transformation:</strong></p>
          <div class="ascii-art">
            Input: "The Quick Brown Fox Jumps!" │ ├─ Step 1: Tokenization │ └─>
            ["The", "Quick", "Brown", "Fox", "Jumps"] │ ├─ Step 2: Lowercasing │
            └─> ["the", "quick", "brown", "fox", "jumps"] │ ├─ Step 3: Stopword
            Filtering (remove "the") │ └─> ["quick", "brown", "fox", "jumps"] │
            ├─ Step 4: Length Filtering (all pass >= 2 chars) │ └─> ["quick",
            "brown", "fox", "jumps"] │ └─ Step 5: Stemming ("jumps" → "jump")
            └─> ["quick", "brown", "fox", "jump"]
          </div>
        </div>
      </section>

      <section>
        <h2>Search Operations</h2>

        <h3>1. Basic Term Search</h3>
        <p>Find all occurrences of a single term:</p>
        <pre><code class="language-go">idx := blaze.NewInvertedIndex()
idx.Index(1, "the quick brown fox")
idx.Index(2, "quick brown dogs")

// Find first occurrence of "quick"
pos, err := idx.First("quick")
if err == nil {
    fmt.Printf("Found at Doc %d, Pos %d\n",
        int(pos.DocumentID), int(pos.Offset))
}

// Find next occurrence
nextPos, _ := idx.Next("quick", pos)</code></pre>

        <h3>2. Phrase Search</h3>
        <p>Find exact sequences of words:</p>
        <pre><code class="language-go">// Find documents containing "quick brown fox" as a phrase
matches := idx.FindAllPhrases("quick brown fox", blaze.BOFDocument)

for _, match := range matches {
    start, end := match[0], match[1]
    fmt.Printf("Found in Doc %d from Pos %d to %d\n",
        int(start.DocumentID), int(start.Offset), int(end.Offset))
}</code></pre>

        <p><strong>Phrase Search Algorithm:</strong></p>
        <div class="ascii-art">
          Searching for phrase: "brown fox" Document: "the quick brown dog
          jumped over the brown fox" Positions: 0 1 2 3 4 5 6 7 8 Phase 1: Find
          END (last word "fox")
          ┌─────────────────────────────────────────────────────────┐ │ Find
          "brown" → Doc:Pos2 │ │ Find "fox" after Pos2 → Doc:Pos8 ← END position
          │ └─────────────────────────────────────────────────────────┘ Phase 2:
          Walk BACKWARDS from END to find START
          ┌─────────────────────────────────────────────────────────┐ │ From
          Pos9, find previous "brown" → Doc:Pos7 ← START │
          └─────────────────────────────────────────────────────────┘ Phase 3:
          Validate ┌─────────────────────────────────────────────────────────┐ │
          Start: Pos7, End: Pos8 │ │ Distance: 8 - 7 = 1 │ │ Expected: 2 words -
          1 = 1 ✓ MATCH! │ │ │ │ "brown" "fox" │ │ ▲ ▲ │ │ Pos7 Pos8
          (consecutive positions) │
          └─────────────────────────────────────────────────────────┘ Steps: 1.
          Find END: Locate the last word of the phrase 2. Walk BACKWARDS: Find
          previous occurrences of earlier words 3. Validate: Check if positions
          are consecutive 4. Recurse: Continue searching for more matches
        </div>

        <h3>3. Proximity Search</h3>
        <p>
          Find documents containing all terms (not necessarily consecutive):
        </p>
        <pre><code class="language-go">// Find documents with both "quick" and "fox"
cover := idx.NextCover([]string{"quick", "fox"}, blaze.BOFDocument)
start, end := cover[0], cover[1]

// Calculate proximity score
distance := end.Offset - start.Offset
score := 1.0 / distance  // Closer terms = higher score</code></pre>

        <p><strong>Cover Algorithm:</strong></p>
        <div class="ascii-art">
          Searching for: ["quick", "fox"] (any order, not necessarily
          consecutive) Document: "the quick brown dog jumped over the lazy fox"
          Positions: 0 1 2 3 4 5 6 7 8 Phase 1: Find COVER END (furthest term)
          ┌──────────────────────────────────────────────────────────────┐ │
          Find "quick" after BOF → Doc:Pos1 │ │ Find "fox" after BOF → Doc:Pos8
          ← FURTHEST (cover end) │
          └──────────────────────────────────────────────────────────────┘ Phase
          2: Find COVER START (earliest term before end)
          ┌──────────────────────────────────────────────────────────────┐ │
          Find "quick" before Pos9 → Doc:Pos1 ← EARLIEST (cover start)│ │ Find
          "fox" before Pos9 → Doc:Pos8 │
          └──────────────────────────────────────────────────────────────┘ Phase
          3: Validate & Return
          ┌──────────────────────────────────────────────────────────────┐ │
          Cover: [Pos1, Pos8] │ │ Same document? ✓ │ │ All terms present? ✓ │ │
          │ │ "quick" ... ... ... ... ... ... ... "fox" │ │ ▲ ▲ │ │ Pos1 Pos8 │
          │ └────────── Cover Range ──────────────┘ │ │ │ │ Proximity Score: 1 /
          (8 - 1 + 1) = 1/8 = 0.125 │
          └──────────────────────────────────────────────────────────────┘
          Steps: 1. Find FURTHEST occurrence of any term (cover end) 2. Find
          EARLIEST occurrence of each term before end (cover start) 3. Validate
          all terms are in the same document 4. Return [start, end] positions
        </div>

        <h3>4. Proximity Ranking</h3>
        <p>Score and rank documents by term proximity:</p>
        <pre><code class="language-go">// Search and rank results
matches := idx.RankProximity("machine learning", 10)

for _, match := range matches {
    fmt.Printf("Doc %d: Score %.2f\n",
        int(match.Offsets[0].DocumentID),
        match.Score)
}</code></pre>

        <div class="highlight-box">
          <p><strong>Scoring Formula:</strong></p>
          <p>
            For each cover in a document:<br />
            <code>score += 1 / (coverEnd - coverStart + 1)</code>
          </p>
        </div>

        <p><strong>Proximity Scoring Examples:</strong></p>
        <div class="ascii-art">
          ┌────────────────────────────────────────────────────────────────┐ │
          Proximity Scoring Examples │
          ├────────────────────────────────────────────────────────────────┤ │ │
          │ Doc 1: "machine learning is machine learning" │ │ Pos:0 1 2 3 4 │ │
          │ │ Cover 1: [Pos 0-1] → score += 1/(1-0+1) = 1/2 = 0.500 │ │ Cover 2:
          [Pos 3-4] → score += 1/(4-3+1) = 1/2 = 0.500 │ │
          ───────────────────────────── │ │ Total Score: 1.000 │ │ │
          ├────────────────────────────────────────────────────────────────┤ │ │
          │ Doc 2: "learning about machine and learning" │ │ Pos:0 1 2 3 4 │ │ │
          │ Cover 1: [Pos 0-2] → score += 1/(2-0+1) = 1/3 = 0.333 │ │ Cover 2:
          [Pos 2-4] → score += 1/(4-2+1) = 1/3 = 0.333 │ │
          ───────────────────────────── │ │ Total Score: 0.666 │ │ │
          ├────────────────────────────────────────────────────────────────┤ │ │
          │ Doc 3: "machine ... ... ... ... learning" │ │ Pos:0 1 2 3 4 5 │ │ │
          │ Cover 1: [Pos 0-5] → score += 1/(5-0+1) = 1/6 = 0.167 │ │
          ───────────────────────────── │ │ Total Score: 0.167 │ │ │
          └────────────────────────────────────────────────────────────────┘
          Ranking: Doc 1 (1.000) > Doc 2 (0.666) > Doc 3 (0.167) ▲ ▲ ▲ Terms
          closest Terms medium Terms far apart Why This Works: - Smaller
          distances → larger scores (inverse relationship) - Multiple
          occurrences → higher scores (additive) - Documents with terms close
          together rank higher
        </div>
      </section>

      <section>
        <h2>API Reference</h2>

        <h3>InvertedIndex Methods</h3>

        <h4>NewInvertedIndex</h4>
        <pre><code class="language-go">func NewInvertedIndex() *InvertedIndex</code></pre>
        <p>Creates a new empty inverted index.</p>

        <h4>Index</h4>
        <pre><code class="language-go">func (idx *InvertedIndex) Index(docID int, document string)</code></pre>
        <p>Adds a document to the inverted index. Thread-safe.</p>

        <h4>RankProximity</h4>
        <pre><code class="language-go">func (idx *InvertedIndex) RankProximity(query string, maxResults int) []Match</code></pre>
        <p>
          Performs proximity-based ranking of search results. This is the main
          search function.
        </p>

        <h4>FindAllPhrases</h4>
        <pre><code class="language-go">func (idx *InvertedIndex) FindAllPhrases(query string, startPos Position) [][]Position</code></pre>
        <p>Finds all occurrences of a phrase in the entire index.</p>

        <h4>NextCover</h4>
        <pre><code class="language-go">func (idx *InvertedIndex) NextCover(tokens []string, startPos Position) []Position</code></pre>
        <p>Finds the next "cover" - a range containing all given tokens.</p>

        <h4>Encode / Decode</h4>
        <pre><code class="language-go">func (idx *InvertedIndex) Encode() ([]byte, error)
func (idx *InvertedIndex) Decode(data []byte) error</code></pre>
        <p>Serializes and deserializes the index to/from binary format.</p>

        <h3>Text Analysis</h3>

        <h4>Analyze</h4>
        <pre><code class="language-go">func Analyze(text string) []string</code></pre>
        <p>
          Transforms raw text into searchable tokens using the default pipeline.
        </p>

        <h4>AnalyzeWithConfig</h4>
        <pre><code class="language-go">func AnalyzeWithConfig(text string, config AnalyzerConfig) []string</code></pre>
        <p>Transforms text using a custom configuration.</p>
        <pre><code class="language-go">config := blaze.AnalyzerConfig{
    MinTokenLength:  3,      // Only keep tokens >= 3 chars
    EnableStemming:  false,  // Disable stemming
    EnableStopwords: true,   // Keep stopword filtering
}
tokens := blaze.AnalyzeWithConfig("The quick brown fox", config)</code></pre>
      </section>

      <section>
        <h2>Performance</h2>

        <h3>Time Complexity</h3>
        <table>
          <tr>
            <th>Operation</th>
            <th>Average</th>
            <th>Worst Case</th>
            <th>Notes</th>
          </tr>
          <tr>
            <td>Index (per document)</td>
            <td>O(n × log m)</td>
            <td>O(n × m)</td>
            <td>n = tokens, m = total positions</td>
          </tr>
          <tr>
            <td>Term lookup</td>
            <td>O(log m)</td>
            <td>O(m)</td>
            <td>m = positions for term</td>
          </tr>
          <tr>
            <td>Phrase search</td>
            <td>O(k × log m)</td>
            <td>O(k × m)</td>
            <td>k = phrase length</td>
          </tr>
          <tr>
            <td>Proximity ranking</td>
            <td>O(t × m)</td>
            <td>O(t × m)</td>
            <td>t = query terms</td>
          </tr>
        </table>

        <h3>Benchmarks</h3>
        <p>Performance on Apple M2 (8 cores), Go 1.24:</p>
        <pre><code>BenchmarkIndex-8                50000    35421 ns/op    18234 B/op    245 allocs/op
BenchmarkTermSearch-8          300000     4123 ns/op      128 B/op      3 allocs/op
BenchmarkPhraseSearch-8        100000    12456 ns/op      512 B/op     12 allocs/op
BenchmarkProximityRanking-8     50000    28934 ns/op     2048 B/op     45 allocs/op
BenchmarkSkipListInsert-8     3000000      413 ns/op      255 B/op      6 allocs/op
BenchmarkSkipListSearch-8     5000000      203 ns/op       23 B/op      1 allocs/op</code></pre>

        <h3>Scalability</h3>
        <table>
          <tr>
            <th>Documents</th>
            <th>Terms</th>
            <th>Index Time</th>
            <th>Search Time</th>
            <th>Memory</th>
          </tr>
          <tr>
            <td>1K</td>
            <td>10K</td>
            <td>50ms</td>
            <td>0.5ms</td>
            <td>2 MB</td>
          </tr>
          <tr>
            <td>10K</td>
            <td>100K</td>
            <td>500ms</td>
            <td>1ms</td>
            <td>20 MB</td>
          </tr>
          <tr>
            <td>100K</td>
            <td>1M</td>
            <td>5s</td>
            <td>2ms</td>
            <td>200 MB</td>
          </tr>
          <tr>
            <td>1M</td>
            <td>10M</td>
            <td>50s</td>
            <td>5ms</td>
            <td>2 GB</td>
          </tr>
        </table>
      </section>

      <section>
        <h2>Use Cases</h2>

        <h3>1. Document Search Systems</h3>
        <pre><code class="language-go">type Document struct {
    ID      int
    Title   string
    Content string
}

func IndexDocuments(docs []Document) *blaze.InvertedIndex {
    idx := blaze.NewInvertedIndex()
    for _, doc := range docs {
        text := doc.Title + " " + doc.Content
        idx.Index(doc.ID, text)
    }
    return idx
}</code></pre>

        <h3>2. Log Analysis</h3>
        <pre><code class="language-go">func IndexLogs(logFile string) (*blaze.InvertedIndex, error) {
    idx := blaze.NewInvertedIndex()
    file, err := os.Open(logFile)
    if err != nil {
        return nil, err
    }
    defer file.Close()

    scanner := bufio.NewScanner(file)
    lineNumber := 1
    for scanner.Scan() {
        idx.Index(lineNumber, scanner.Text())
        lineNumber++
    }
    return idx, scanner.Err()
}</code></pre>

        <h3>3. E-commerce Product Search</h3>
        <pre><code class="language-go">type Product struct {
    ID          int
    Name        string
    Description string
    Category    string
    Tags        []string
}

func IndexProducts(products []Product) *blaze.InvertedIndex {
    idx := blaze.NewInvertedIndex()
    for _, product := range products {
        searchText := fmt.Sprintf("%s %s %s %s",
            product.Name,
            product.Description,
            product.Category,
            strings.Join(product.Tags, " "))
        idx.Index(product.ID, searchText)
    }
    return idx
}</code></pre>
      </section>

      <section>
        <h2>Configuration</h2>

        <h3>Analyzer Configuration</h3>
        <p>Customize the text analysis pipeline:</p>
        <pre><code class="language-go">type AnalyzerConfig struct {
    MinTokenLength  int  // Minimum token length (default: 2)
    EnableStemming  bool // Apply stemming (default: true)
    EnableStopwords bool // Remove stopwords (default: true)
}</code></pre>

        <div class="highlight-box">
          <p><strong>Configuration Examples:</strong></p>
          <p>
            <strong>Exact matching:</strong> No stemming, keep all words<br />
            <strong>Fuzzy matching:</strong> Aggressive stemming<br />
            <strong>Code search:</strong> No stemming, no stopwords, longer
            tokens
          </p>
        </div>
      </section>

      <section>
        <h2>Best Practices</h2>

        <ol>
          <li>
            <strong>Use stable document IDs</strong> - Use database primary
            keys, not array indices
          </li>
          <li>
            <strong>Batch indexing for large datasets</strong> - Process in
            batches with periodic checkpoints
          </li>
          <li>
            <strong>Choose appropriate analyzer config</strong> - Match
            configuration to your use case
          </li>
          <li>
            <strong>Persist index for large datasets</strong> - Don't rebuild
            the index every time
          </li>
          <li>
            <strong>Handle concurrent access</strong> - Use RWMutex for
            read-heavy workloads
          </li>
          <li>
            <strong>Limit result set size</strong> - Always specify a reasonable
            max results
          </li>
          <li>
            <strong>Pre-process queries</strong> - Normalize queries before
            searching
          </li>
        </ol>
      </section>

      <section>
        <h2>Architecture</h2>

        <h3>Component Overview</h3>
        <pre><code>blaze/
├── index.go          # Inverted index implementation
├── skiplist.go       # Skip list data structure
├── search.go         # Search algorithms (phrase, proximity)
├── analyzer.go       # Text analysis pipeline
├── serialization.go  # Binary encoding/decoding
├── *_test.go         # Comprehensive test suite
├── Makefile          # Development commands
└── public/           # Documentation website
    └── index.html</code></pre>

        <h3>Data Flow</h3>
        <p>Complete data flow from user input to ranked search results:</p>
        <div class="ascii-art">
          ┌──────────────────────────────────────────────────────────────────────┐
          │ Complete Data Flow │
          └──────────────────────────────────────────────────────────────────────┘
          User Input "The Quick Brown Fox!" │ ▼
          ┌───────────────────────────────────────────┐ │ Text Analysis Pipeline
          │ │ ┌─────────────────────────────────────┐ │ │ │ 1. Tokenization │ │
          │ │ ["The", "Quick", "Brown", "Fox"] │ │ │
          └────────────┬────────────────────────┘ │ │ ▼ │ │
          ┌─────────────────────────────────────┐ │ │ │ 2. Lowercasing │ │ │ │
          ["the", "quick", "brown", "fox"] │ │ │
          └────────────┬────────────────────────┘ │ │ ▼ │ │
          ┌─────────────────────────────────────┐ │ │ │ 3. Stopword Filtering │
          │ │ │ ["quick", "brown", "fox"] │ │ │
          └────────────┬────────────────────────┘ │ │ ▼ │ │
          ┌─────────────────────────────────────┐ │ │ │ 4. Length Filtering │ │
          │ │ ["quick", "brown", "fox"] │ │ │
          └────────────┬────────────────────────┘ │ │ ▼ │ │
          ┌─────────────────────────────────────┐ │ │ │ 5. Stemming │ │ │ │
          ["quick", "brown", "fox"] │ │ │
          └────────────┬────────────────────────┘ │
          └───────────────┼────────────────────────────┘ ▼ ["quick", "brown",
          "fox"] │ ▼ ┌───────────────────────────────────────────┐ │ Inverted
          Index (Indexing) │ │ │ │ ┌─────────┬────────────────────────┐ │ │ │
          "quick" │ → SkipList │ │ │ │ │ └─> [Doc1:Pos0] │ │ │
          ├─────────┼────────────────────────┤ │ │ │ "brown" │ → SkipList │ │ │
          │ │ └─> [Doc1:Pos1] │ │ │ ├─────────┼────────────────────────┤ │ │ │
          "fox" │ → SkipList │ │ │ │ │ └─> [Doc1:Pos2] │ │ │
          └─────────┴────────────────────────┘ │
          └───────────────┬───────────────────────────┘ │
          ┌─────────────────┴─────────────────┐ │ Search Operations │ ▼ ▼
          ┌──────────┐ ┌────────────┐ │ Term │ │ Phrase │ │ Search │ │ Search │
          └────┬─────┘ └─────┬──────┘ │ │ └──────────┬───────────────────────┘ ▼
          ┌───────────────┐ │ Proximity │ │ Ranking │ └───────┬───────┘ │ ▼
          ┌───────────────────────┐ │ Ranked Results │ │ ┌─────────────────┐ │ │
          │ Doc 1: Score 1.0│ │ │ │ Doc 2: Score 0.5│ │ │ │ Doc 3: Score 0.3│ │
          │ └─────────────────┘ │ └───────────────────────┘
        </div>

        <h3>Key Design Decisions</h3>

        <h4>1. Skip Lists over Balanced Trees</h4>
        <ul>
          <li>Simpler implementation (no rotation logic)</li>
          <li>Better cache locality</li>
          <li>Easier to make concurrent</li>
          <li>Comparable performance (O(log n))</li>
          <li>Used in production systems (Redis, LevelDB)</li>
        </ul>

        <h4>2. Position-Based Indexing</h4>
        <p>
          Instead of just tracking document IDs, Blaze tracks exact word
          positions, enabling:
        </p>
        <div class="ascii-art">
          Traditional Index (Document IDs only): ┌─────────┬──────────────────┐
          │ "quick" │ [Doc1, Doc3] │ ✗ Can't do phrase search
          └─────────┴──────────────────┘ ✗ Can't rank by proximity
          Position-Based Index (Document + Offset):
          ┌─────────┬────────────────────────────────────┐ │ "quick" │
          [Doc1:Pos1, Doc3:Pos0] │ ✓ Phrase search │ "brown" │ [Doc1:Pos2,
          Doc3:Pos1] │ ✓ Proximity ranking │ "fox" │ [Doc1:Pos3] │ ✓ Snippet
          generation └─────────┴────────────────────────────────────┘ ✓ Precise
          results Can verify: "quick brown" is a phrase in Doc1 (Pos1→Pos2) but
          NOT in Doc3 (Pos0 and Pos1 are not "quick brown")
        </div>
        <ul>
          <li>Phrase search (check consecutive positions)</li>
          <li>Proximity ranking (measure distances)</li>
          <li>Snippet generation (extract relevant parts)</li>
          <li>More precise search results</li>
        </ul>
        <p><strong>Trade-offs:</strong></p>
        <ul>
          <li>Larger index size (~2-3x more data)</li>
          <li>More complex algorithms (but still O(log n))</li>
        </ul>

        <h4>3. Binary Serialization</h4>
        <ul>
          <li>60% smaller file size compared to JSON</li>
          <li>3x faster parsing</li>
          <li>Preserves skip list structure</li>
          <li>Suitable for large indexes</li>
        </ul>
      </section>

      <section>
        <h2>Testing</h2>
        <pre><code class="language-bash"># Run all tests
make test

# Run tests with coverage
make test-coverage

# Run benchmarks
make bench

# Run all checks (format, vet, lint, test)
make check</code></pre>

        <div class="highlight-box">
          <p><strong>Test Coverage:</strong></p>
          <ul style="margin-top: 0.5rem">
            <li>Inverted Index: 100%</li>
            <li>Skip Lists: 100%</li>
            <li>Text Analysis: 100%</li>
            <li>Search Operations: 98%</li>
            <li>Serialization: 100%</li>
          </ul>
        </div>
      </section>

      <section>
        <h2>Contributing</h2>
        <p>Contributions are welcome! Please follow these guidelines:</p>

        <h3>Development Setup</h3>
        <pre><code class="language-bash"># Clone repository
git clone https://github.com/wizenheimer/blaze.git
cd blaze

# Install dependencies
make deps

# Run tests
make test

# Run linter
make lint</code></pre>

        <h3>Code Style</h3>
        <ul>
          <li>Follow Go conventions (gofmt, golint)</li>
          <li>Write comprehensive comments</li>
          <li>Include examples in documentation</li>
          <li>Add tests for new features</li>
          <li>Keep functions focused and small</li>
        </ul>
      </section>

      <footer>
        <p>Built by <a href="https://github.com/wizenheimer">wizenheimer</a></p>
        <p style="margin-top: 0.5rem">
          Inspired by Elasticsearch, Lucene, Redis, and LevelDB
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
  </body>
</html>
